%% LyX 2.0.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}
\usepackage{breakurl}


\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% ---- ETD Document Class and Useful Packages ---- %

\usepackage{subfigure}\usepackage{epsfig}\usepackage{amsfonts}\usepackage{bigints}\usepackage{amsthm}\usepackage{algorithmic}\usepackage{algorithm}\usepackage{caption}\usepackage{fullpage}\usepackage{graphicx}\usepackage{array}

%% Use these commands to set biographic information for the title page:
\title{Switchboard Spoken Term Detection Experiments}
\author{Mark Stoehr}
\date{\today}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\makeatother

\begin{document}

\section{Introduction}

We are attempting to build models of images where we assume that we
have an object of fixed size surrounded by background. Concretely,
we suppose that the image is modeled by a regularly sampled lattice
$L$ (usually we will work in a $2d$ interval subset of $\mathbb{Z}^{2}$
which is our model of the time-frequency plane). At each location
$x\in L$ we observe a binary feature vector $\{0,1\}^{n}$ which
will usually contain the edge information (in the case of edges $n=8$
for four directions with two polarities each). A given image $X=\{X_{e}(x)\mid x\in L,e=1,\ldots,E\}$
where $X_{e}(x)\in\{0,1\}$. Our data model is a probability array
$(p_{e}(x))_{x\in L}$ where each $p_{e}(x)\in(0,1)$ is the probability
of observing feature $e$ at location $x$. We parameterize the probability
array as representing an object embedded in background so that probabilities
$p_{e}(x)$ such that $x\in W\subset L$ are considered to be within
the object window and $p_{e}(x)\not\in W$ are considered background
edge probabilities. The distinction between object and background
in this case is that the background is modeled as uniform across the
spatial axes whereas the object is modeled using a template 
\[
Q=(p_{e}(s))_{s\in W},e=1,\ldots,E.
\]
 We may write background probabilities as $p_{e,\text{bgd}}$. We
model each feature as being conditionally independent given the location
and object model. For a given observation $X$ we write the likelihood
as
\begin{eqnarray*}
P(X) & = & \prod_{x\in W}\prod_{e=1}^{E}p_{e}(x)^{X_{e}(x)}(1-p_{e}(x))^{1-X_{e}(x)}\\
 &  & \cdot\prod_{x\in L\setminus W}\prod_{e=1}^{E}p_{e,\text{bgd}}{}^{X_{e}(x)}(1-p_{e,\text{bgd}})^{1-X_{e}(x)}.
\end{eqnarray*}



\section{A Shiftable Model}

In the real world the object is often not perfectly centered in our
lattice so we want to develop methods that are robust to shifts. In
this setting the observations are $(X,R)$ where $R$ is the reference
location in $L$ indicating where the data have been shifted to. In
this case the conditional likelihood may be written
\begin{eqnarray*}
P(X\mid r;Q,p_{\text{bgd}}) & = & \prod_{x\in W}\prod_{e=1}^{E}p_{e}(x)^{X_{e}(x+r)}(1-p_{e}(x))^{1-X_{e}(x+r)}\\
 &  & \cdot\prod_{x\in L\setminus(W+r)}\prod_{e=1}^{E}p_{e,\text{bgd}}{}^{X_{e}(x)}(1-p_{e,\text{bgd}})^{1-X_{e}(x)}
\end{eqnarray*}
where $W+r=\{s+r\mid s\in W\}$ and we can see that we simply have
shifted the template $Q$ by $r$ positions on the lattice $L$. The
probability distribution over shifts is assumed to be $(\tau_{r})$
so that the fully observed likelihood is
\begin{eqnarray*}
P(X,r) & = & \sum_{r}\tau_{r}P(X\mid r).
\end{eqnarray*}
 A basic extension of this model is to also allow for multiple classes
so that we have a distinct template $Q_{c}$ for each class $c$,
as well as a distribution $P(c)$ that gives the prior distribution
over each class. If we assume that different classes have different
shift distributions we can let $\pi(r,c)=P(r,c)$ parameterize the
joint distribution over classes and shifts. The likelihood is then
\[
P(X,R,C;\{Q_{c}\},p_{\text{bgd}})=\sum_{r,c}\pi(r,c)P(X\mid r;Q_{c}).
\]
 In the multi-class setting the probability array for $Q_{c}$ has
elements $p_{e,c}(s)$ for locations $s\in L.$


\section{EM model estimation}

We now consider the problem of parameter estimation using the expectation-maximization
algorithm. Let $\{(X^{(j)},V^{(j)})\}_{j=1}^{J}$ be our data we wish
to estimate $Q$, $\pi$, and $p_{\text{bgd}}$ where $V^{(j)}=(R^{(j)},C^{(j)})$
indicates the shift and class. We will denote by $v=(r,c)$ particular
values for realizations of the random variable $V$. The EM-algorithm
is an iterative algorithm where given estimates $Q^{(k)}$, $\pi^{(k)}$,
$p_{\text{bgd}}^{(k)}$ we get estimates $Q^{(k+1)}$, $\pi^{(k+1)}$,
$p_{\text{bgd}}^{(k+1)}$ consists in two-steps:
\begin{enumerate}
\item Estimation: compute the posterior distribution over the shifts and
classes under the current set of parameters 
\begin{equation}
P(v\mid X^{(j)};Q^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)})=\frac{P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)}{\sum_{v'}P(X^{(j)}\mid v';Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v')}\label{eq:e-step}
\end{equation}
 
\item Maximization: estimate new sets of parameters $Q^{(k+1)},\pi^{(k+1)},p_{\text{bgd}}^{(k+1)}$
by maximizing that expected log-likelihood using
\begin{eqnarray*}
\pi^{(k+1)}(v) & = & \frac{1}{J}\sum_{j}P(v\mid X^{(j)};Q^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)})\\
p_{e,c}^{(k+1)}(s) & = & \frac{1}{J}\sum_{r,c}\sum_{j}P(r,c\mid X^{(j)};Q_{c}^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)})X_{e}^{(j)}(s+r),\; s\in W\\
p_{e,\text{bgd}} & = & \frac{1}{J\cdot N_{\text{bgd}}}\sum_{r,c}\sum_{j}\sum_{s\in L\setminus(W+r)}P(v\mid X^{(j)};Q_{c}^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)})X_{e}^{(j)}(s)
\end{eqnarray*}
where $N_{\text{bgd}}$ is the number of background points $|L\setminus W|$.
\end{enumerate}
Initialization is done by going directly to step two, the maximization
step, with randomly set $P(v\mid X^{(j)};Q^{(0)},\pi^{(0)},p_{\text{bgd}}^{(0)})$.
The random assignment is that $P((0,c)\mid X^{(j)};Q^{(0)},\pi^{(0)},p_{\text{bgd}}^{(0)})$
is one with probability $\frac{1}{N_{\text{class}}}$ and zero otherwise
where $N_{\text{class}}$ is the number of classes and for $r\neq0$
we set $P((r,c)\mid X^{(j)};Q^{(0)},\pi^{(0)},p_{\text{bgd}}^{(0)})$
is zero. Convergence is assessed by whether the improvement in likelihood
is greater than some threshold 
\[
\frac{l^{(k+1)}-l^{(k)}}{|l^{(k)}|}>\epsilon
\]
where $\epsilon$ is a tolerance parameter and $l^{(k)}$ is the data
likelihood
\[
l^{(k)}=\sum_{j}\sum_{v}P(X^{(j)}\mid v;Q_{c}^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v).
\]
We know from the theory of EM that $l^{(k+1)}\geq l^{(k)}$.

In general the data is very high dimensional and it is more convenient
to work in the log-likelihood domain rather than with raw likelihoods.
In particular, \ref{eq:e-step} is numerically unstable to compute
naively so we instead compute the E-step as follows:
\begin{eqnarray*}
P(v\mid X^{(j)};Q^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)}) & = & \frac{\exp\log\left(P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)\right)}{\exp\log\sum_{v'}P(X^{(j)}\mid v';Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v')}\\
 & = & \frac{\exp\left(\log\left(P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)\right)-\alpha^{(j,k)}\right)}{\exp\left(-\alpha^{(j,k)}\right)\sum_{v'}P(X^{(j)}\mid v';Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v')}\\
 & = & \frac{\exp\left(\log\left(P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)\right)-\alpha^{(j,k)}\right)}{\sum_{v'}\exp\left(\log\left(P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)\right)-\alpha^{(j,k)}\right)}
\end{eqnarray*}
where 
\[
\alpha^{(j,k)}=\max_{v}\log P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)
\]
and this ensures that the numerator is exactly one for some $v$ (and
generally close to zero for all other $v'$), and that the denominator
is always between one and the size of the shift space: $|\{v\}|$.


\section{Experimental Results}

We illustrate the usefulness of the proposed model on synthetic and real datasets.  Full code
for reproducing the experiments is available on the author's website.
 
\subsection{Recovering Shifts in Synthetic Data}

This experiment may be run from the root directory with the 
command
\begin{verbatim}
scripts/synthetic.sh .
\end{verbatim}
in this experiment we generated random binary data using
templates in \autoref{fig:synthetic_templates}.
\begin{figure}[h]
\centering
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[scale=.25]{data/generated_templates_0.png}}
\fbox{\includegraphics[scale=.25]{data/generated_templates_1.png}}
\caption{Generated templates}
\label{fig:synthetic_templates}
\end{figure}
and these may be compared to the templates inferred using the
EM algorithm. In generating the data with these templates we also
add random Bernoulli noise so that the data look like \autoref{fig:generated_data}
\begin{figure}[h]
\centering
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[scale=.15]{data/X_0.png}}
\fbox{\includegraphics[scale=.15]{data/X_1.png}}
\fbox{\includegraphics[scale=.15]{data/X_2.png}}
\fbox{\includegraphics[scale=.15]{data/X_3.png}}
\fbox{\includegraphics[scale=.15]{data/X_4.png}}
\fbox{\includegraphics[scale=.15]{data/X_5.png}}
\caption{Generated Data}
\label{fig:generated_data}
\end{figure}
 In the first case the number of allowed shifts
was set to one, so that no shifting took place and we get these
templates in \autoref{fig:no_shift_templates}
\begin{figure}[h]
\centering
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[scale=.25]{shift_class_lengths_exp/out_no_shift_templates_0.png}}
\fbox{\includegraphics[scale=.25]{shift_class_lengths_exp/out_no_shift_templates_1.png}}
\caption{Templates inferred without shifts}
\label{fig:no_shift_templates}
\end{figure}
which are considerably noisier than these templates in
\autoref{fig:shift_spike_templates} which do not have
\begin{figure}[h]
\centering
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[scale=.25]{shift_class_lengths_exp/out_shift_spike_templates_0.png}}
\fbox{\includegraphics[scale=.25]{shift_class_lengths_exp/out_shift_spike_templates_1.png}}
\caption{Templates inferred with shifts}
\label{fig:shift_spike_templates}
\end{figure}
the smearing evident in \autoref{fig:no_shift_templates} since
we allowed shifts during training.



\subsection{Experiment with p versus b}

In this next experiment we compare a classification experiment
with binary edge feaures computed over spectrograms of individuals
saying \texttt{p} versus \texttt{b}.  The labels and recording
came from the TIMIT databse.

We find that there is good performance to be had in classification
when using roughly six templates for each class. We first
establish that longer or shorter templates do not make much 
of a difference at the scale that we are working at, this is 
important because it shows that the information in the boundary
is not all that important for classification and can be
coherently modeled as background.

We can see the result in 
\begin{table}[h]
  \centering
  \begin{tabular}{| l | c | c |  r |}
    \hline
     Frame Length & Shifts & Other Info & Error Rate \\ \hline\hline
     31 & 1 &  & \input{exp/p_b_exp/b_p_1sh_6c_3st_31l.error_rate} \\
     \hline
     37 & 1 &  & \input{exp/p_b_exp/b_p_1sh_6c_0st_37l.error_rate} \\
     \hline
     35 & 3 &  & \input{exp/p_b_exp/b_p_3sh_6c_0st_35l.error_rate} \\
     \hline
     35 & 1 &  & \input{exp/p_b_exp/b_p_1sh_6c_1st_35l.error_rate} \\
     \hline
     31 & 7 &  & \input{exp/p_b_exp/b_p_7sh_6c_0st_31l.error_rate} \\
     \hline
     31 & 7 & \texttt{prev-init} & \input{exp/p_b_exp/b_p_7sh_6c_0st_31l_previnit.error_rate} \\
     \hline
     31 & 5  & \texttt{prev-init}  & \input{exp/p_b_exp/b_p_5sh_6c_1st_31l_previnit.error_rate} \\
     \hline
     31 & 5 & \texttt{prev-init},NI & \input{exp/p_b_exp/b_p_5sh_6c_1st_31l_previnit_ni.error_rate} \\
     \hline
     31 & 7 & NI & \input{exp/p_b_exp/b_p_7sh_6c_0st_31l_normal_independent.error_rate} \\
     \hline
     31 & 7 & ND & \input{exp/p_b_exp/b_p_7sh_6c_0st_31l_normal_dependent.error_rate} \\
     \hline
  \end{tabular}
  \caption{Error rates for shorter and longer templates}
  \label{tab:myfirsttable}
\end{table}
and so we see a non-linear relationship between shifts and error
rates.  It seems that allowing enough shifts (and using a good 
enough initialization) we can equal the performance of the model
without the shift.  On the other hand, we do worse with
badly initialized shifting models or models which do not
allow enough shifts. A word on the codes: \texttt{prev-init} indicates
that we use the model developed with no shifts as the initialization of the templates.  While `NI` indicates that we enforce a 
normal-like distribution over shifts that is independent of class,
so that, contrary to the description in the previous sections we
have $\pi(r,c)=\tau(r)\eta(c) $ where $\tau$ is the normal-like
distribution over shifts and $\eta$ is a distribution over classes.
The estimation equations for $\tau$ and $\eta$ simplify involve
marginalizing over classes and shifts, respectively. 

We also visualize these templates
and then we see them with shifts
\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_5.png}

\caption{Spectrogram visualization of the \texttt{p} templates with no shifts}
\label{fig:underlying_1sh_6c_3st_31l_templates}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_5.png}

\caption{Spectrogram visualization of the \texttt{p} templates with seven shifts}
\label{fig:underlying_7sh_6c_0st_31l_templates}
\end{figure}

and then we see what happens when we have a good initialization

\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_5.png}

\caption{Spectrogram visualization of the \texttt{p} templates with seven shifts and a good inititalization}
\label{fig:underlying_7sh_6c_0st_31l_previnit_templates}
\end{figure}


\section{\texttt{let} Experiment}

In this experiment we are working on detecting individuals saying
\texttt{let} in switchboard this is because it is not so frequent
that it is guaranteed to be in every sentence and its short enough
that an inflexible template should be able to handle the length variation (since 90\% of the examples are shorter than quarter of a second) . However, we do not have good
transcriptions since the alignment was done automatically
using a forced aligner.  The first parameter that needs to be
set is the template length, since all of our algorithms
need a length  as an input.  The length parameter is essentially
an upperbound on the length of the model. From the histogram
in \autoref{fig:train_let_lengths_hist}
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/train_let_lengths_hist.png}
\caption{Histogram over the lengths of \texttt{let} in swbd}
\label{fig:train_let_lengths_hist}
\end{figure}
we see that \texttt{.4} seconds will cover most of the instances.
We can include a longer template if need be.  We now examine
the templates learned when we extract the data and train templates
over the specified length.  There are notably two strategies 
to extracting a data window of fixed length: the first is to extract
the window from the hypothesized beginning and the second
is to extract the window from the hypothesized middle.  We opt
for the latter type and will allow shifts so we extracting a .5 second window around the middle of the data.
To get a sense of the different templates that are learned over
\texttt{let}
here we can see a 4-class visualization of templates
trained with one shift in \autoref{fig:let_underlying_viz_1sh_4c_3st_54l}, 
three shifts in \autoref{fig:let_underlying_viz_3sh_4c_0st_58l_prev_init}, 
and then seven shifts in \autoref{fig:let_underlying_viz_7sh_4c_0st_54l_prev_init}:
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_underlying_templates_0.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_underlying_templates_1.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_underlying_templates_2.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_underlying_templates_3.png}
\caption{Visualization of let-clusters with 1 shift}
\label{fig:let_underlying_viz_1sh_4c_3st_54l}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_3sh_4c_0st_58l_prev_init_underlying_templates_0.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_3sh_4c_0st_58l_prev_init_underlying_templates_1.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_3sh_4c_0st_58l_prev_init_underlying_templates_2.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_3sh_4c_0st_58l_prev_init_underlying_templates_3.png}
\caption{Visualization of let-clusters with 3 shifts}
\label{fig:let_underlying_viz_3sh_4c_0st_58l_prev_init}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_underlying_templates_0.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_underlying_templates_1.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_underlying_templates_2.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_underlying_templates_3.png}
\caption{Visualization of let-clusters with 7 shifts}
\label{fig:let_underlying_viz_7sh_4c_0st_54l_prev_init}
\end{figure}
and then looking at the edge templates with seven shifts (\autoref{fig:let_7sh_4c_0st_54l_prev_init_templates}) compared with 1 shift (\autoref{fig:let_1sh_4c_3st_54l_templates})
\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_5.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_6.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_7.png}
\caption{Visualization of let-edge templates with 7 shifts}
\label{fig:let_7sh_4c_0st_54l_prev_init_templates}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_5.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_6.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_7.png}
\caption{Visualization of let-edge templates with 1 shift}
\label{fig:let_1sh_4c_3st_54l_templates}
\end{figure}




\end{document}
