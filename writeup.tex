%% LyX 2.0.2 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{article}
\usepackage[latin9]{inputenc}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage[unicode=true,pdfusetitle,
 bookmarks=true,bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 1},backref=section,colorlinks=false]
 {hyperref}
\usepackage{breakurl}


\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
% ---- ETD Document Class and Useful Packages ---- %

\usepackage{subfigure}\usepackage{epsfig}\usepackage{amsfonts}\usepackage{bigints}\usepackage{amsthm}\usepackage{algorithmic}\usepackage{algorithm}\usepackage{caption}\usepackage{fullpage}\usepackage{graphicx}\usepackage{array}

%% Use these commands to set biographic information for the title page:
\title{Switchboard Spoken Term Detection Experiments}
\author{Mark Stoehr}
\date{\today}

\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}

\makeatother

\begin{document}

\section{Introduction}

We are attempting to build models of images where we assume that we
have an object of fixed size surrounded by background. Concretely,
we suppose that the image is modeled by a regularly sampled lattice
$L$ (usually we will work in a $2d$ interval subset of $\mathbb{Z}^{2}$
which is our model of the time-frequency plane). At each location
$x\in L$ we observe a binary feature vector $\{0,1\}^{n}$ which
will usually contain the edge information (in the case of edges $n=8$
for four directions with two polarities each). A given image $X=\{X_{e}(x)\mid x\in L,e=1,\ldots,E\}$
where $X_{e}(x)\in\{0,1\}$. Our data model is a probability array
$(p_{e}(x))_{x\in L}$ where each $p_{e}(x)\in(0,1)$ is the probability
of observing feature $e$ at location $x$. We parameterize the probability
array as representing an object embedded in background so that probabilities
$p_{e}(x)$ such that $x\in W\subset L$ are considered to be within
the object window and $p_{e}(x)\not\in W$ are considered background
edge probabilities. The distinction between object and background
in this case is that the background is modeled as uniform across the
spatial axes whereas the object is modeled using a template 
\[
Q=(p_{e}(s))_{s\in W},e=1,\ldots,E.
\]
 We may write background probabilities as $p_{e,\text{bgd}}$. We
model each feature as being conditionally independent given the location
and object model. For a given observation $X$ we write the likelihood
as
\begin{eqnarray*}
P(X) & = & \prod_{x\in W}\prod_{e=1}^{E}p_{e}(x)^{X_{e}(x)}(1-p_{e}(x))^{1-X_{e}(x)}\\
 &  & \cdot\prod_{x\in L\setminus W}\prod_{e=1}^{E}p_{e,\text{bgd}}{}^{X_{e}(x)}(1-p_{e,\text{bgd}})^{1-X_{e}(x)}.
\end{eqnarray*}



\section{A Shiftable Model}

In the real world the object is often not perfectly centered in our
lattice so we want to develop methods that are robust to shifts. In
this setting the observations are $(X,R)$ where $R$ is the reference
location in $L$ indicating where the data have been shifted to. In
this case the conditional likelihood may be written
\begin{eqnarray*}
P(X\mid r;Q,p_{\text{bgd}}) & = & \prod_{x\in W}\prod_{e=1}^{E}p_{e}(x)^{X_{e}(x+r)}(1-p_{e}(x))^{1-X_{e}(x+r)}\\
 &  & \cdot\prod_{x\in L\setminus(W+r)}\prod_{e=1}^{E}p_{e,\text{bgd}}{}^{X_{e}(x)}(1-p_{e,\text{bgd}})^{1-X_{e}(x)}
\end{eqnarray*}
where $W+r=\{s+r\mid s\in W\}$ and we can see that we simply have
shifted the template $Q$ by $r$ positions on the lattice $L$. The
probability distribution over shifts is assumed to be $(\tau_{r})$
so that the fully observed likelihood is
\begin{eqnarray*}
P(X,r) & = & \sum_{r}\tau_{r}P(X\mid r).
\end{eqnarray*}
 A basic extension of this model is to also allow for multiple classes
so that we have a distinct template $Q_{c}$ for each class $c$,
as well as a distribution $P(c)$ that gives the prior distribution
over each class. If we assume that different classes have different
shift distributions we can let $\pi(r,c)=P(r,c)$ parameterize the
joint distribution over classes and shifts. The likelihood is then
\[
P(X,R,C;\{Q_{c}\},p_{\text{bgd}})=\sum_{r,c}\pi(r,c)P(X\mid r;Q_{c}).
\]
 In the multi-class setting the probability array for $Q_{c}$ has
elements $p_{e,c}(s)$ for locations $s\in L.$


\section{EM model estimation}

We now consider the problem of parameter estimation using the expectation-maximization
algorithm. Let $\{(X^{(j)},V^{(j)})\}_{j=1}^{J}$ be our data we wish
to estimate $Q$, $\pi$, and $p_{\text{bgd}}$ where $V^{(j)}=(R^{(j)},C^{(j)})$
indicates the shift and class. We will denote by $v=(r,c)$ particular
values for realizations of the random variable $V$. The EM-algorithm
is an iterative algorithm where given estimates $Q^{(k)}$, $\pi^{(k)}$,
$p_{\text{bgd}}^{(k)}$ we get estimates $Q^{(k+1)}$, $\pi^{(k+1)}$,
$p_{\text{bgd}}^{(k+1)}$ consists in two-steps:
\begin{enumerate}
\item Estimation: compute the posterior distribution over the shifts and
classes under the current set of parameters 
\begin{equation}
P(v\mid X^{(j)};Q^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)})=\frac{P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)}{\sum_{v'}P(X^{(j)}\mid v';Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v')}\label{eq:e-step}
\end{equation}
 
\item Maximization: estimate new sets of parameters $Q^{(k+1)},\pi^{(k+1)},p_{\text{bgd}}^{(k+1)}$
by maximizing that expected log-likelihood using
\begin{eqnarray*}
\pi^{(k+1)}(v) & = & \frac{1}{J}\sum_{j}P(v\mid X^{(j)};Q^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)})\\
p_{e,c}^{(k+1)}(s) & = & \frac{1}{J}\sum_{r,c}\sum_{j}P(r,c\mid X^{(j)};Q_{c}^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)})X_{e}^{(j)}(s+r),\; s\in W\\
p_{e,\text{bgd}} & = & \frac{1}{J\cdot N_{\text{bgd}}}\sum_{r,c}\sum_{j}\sum_{s\in L\setminus(W+r)}P(v\mid X^{(j)};Q_{c}^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)})X_{e}^{(j)}(s)
\end{eqnarray*}
where $N_{\text{bgd}}$ is the number of background points $|L\setminus W|$.
\end{enumerate}
Initialization is done by going directly to step two, the maximization
step, with randomly set $P(v\mid X^{(j)};Q^{(0)},\pi^{(0)},p_{\text{bgd}}^{(0)})$.
The random assignment is that $P((0,c)\mid X^{(j)};Q^{(0)},\pi^{(0)},p_{\text{bgd}}^{(0)})$
is one with probability $\frac{1}{N_{\text{class}}}$ and zero otherwise
where $N_{\text{class}}$ is the number of classes and for $r\neq0$
we set $P((r,c)\mid X^{(j)};Q^{(0)},\pi^{(0)},p_{\text{bgd}}^{(0)})$
is zero. Convergence is assessed by whether the improvement in likelihood
is greater than some threshold 
\[
\frac{l^{(k+1)}-l^{(k)}}{|l^{(k)}|}>\epsilon
\]
where $\epsilon$ is a tolerance parameter and $l^{(k)}$ is the data
likelihood
\[
l^{(k)}=\sum_{j}\sum_{v}P(X^{(j)}\mid v;Q_{c}^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v).
\]
We know from the theory of EM that $l^{(k+1)}\geq l^{(k)}$.

In general the data is very high dimensional and it is more convenient
to work in the log-likelihood domain rather than with raw likelihoods.
In particular, \ref{eq:e-step} is numerically unstable to compute
naively so we instead compute the E-step as follows:
\begin{eqnarray*}
P(v\mid X^{(j)};Q^{(k)},\pi^{(k)},p_{\text{bgd}}^{(k)}) & = & \frac{\exp\log\left(P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)\right)}{\exp\log\sum_{v'}P(X^{(j)}\mid v';Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v')}\\
 & = & \frac{\exp\left(\log\left(P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)\right)-\alpha^{(j,k)}\right)}{\exp\left(-\alpha^{(j,k)}\right)\sum_{v'}P(X^{(j)}\mid v';Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v')}\\
 & = & \frac{\exp\left(\log\left(P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)\right)-\alpha^{(j,k)}\right)}{\sum_{v'}\exp\left(\log\left(P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)\right)-\alpha^{(j,k)}\right)}
\end{eqnarray*}
where 
\[
\alpha^{(j,k)}=\max_{v}\log P(X^{(j)}\mid v;Q^{(k)},p_{\text{bgd}}^{(k)})\pi^{(k)}(v)
\]
and this ensures that the numerator is exactly one for some $v$ (and
generally close to zero for all other $v'$), and that the denominator
is always between one and the size of the shift space: $|\{v\}|$.


\section{Experimental Results}

We illustrate the usefulness of the proposed model on synthetic and real datasets.  Full code
for reproducing the experiments is available on the author's website.
 
\subsection{Recovering Shifts in Synthetic Data}

This experiment may be run from the root directory with the 
command
\begin{verbatim}
scripts/synthetic.sh .
\end{verbatim}
in this experiment we generated random binary data using
templates in \autoref{fig:synthetic_templates}.
\begin{figure}[h]
\centering
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[scale=.25]{data/generated_templates_0.png}}
\fbox{\includegraphics[scale=.25]{data/generated_templates_1.png}}
\caption{Generated templates}
\label{fig:synthetic_templates}
\end{figure}
and these may be compared to the templates inferred using the
EM algorithm. In generating the data with these templates we also
add random Bernoulli noise so that the data look like \autoref{fig:generated_data}
\begin{figure}[h]
\centering
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[scale=.15]{data/X_0.png}}
\fbox{\includegraphics[scale=.15]{data/X_1.png}}
\fbox{\includegraphics[scale=.15]{data/X_2.png}}
\fbox{\includegraphics[scale=.15]{data/X_3.png}}
\fbox{\includegraphics[scale=.15]{data/X_4.png}}
\fbox{\includegraphics[scale=.15]{data/X_5.png}}
\caption{Generated Data}
\label{fig:generated_data}
\end{figure}
 In the first case the number of allowed shifts
was set to one, so that no shifting took place and we get these
templates in \autoref{fig:no_shift_templates}
\begin{figure}[h]
\centering
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[scale=.25]{shift_class_lengths_exp/out_no_shift_templates_0.png}}
\fbox{\includegraphics[scale=.25]{shift_class_lengths_exp/out_no_shift_templates_1.png}}
\caption{Templates inferred without shifts}
\label{fig:no_shift_templates}
\end{figure}
which are considerably noisier than these templates in
\autoref{fig:shift_spike_templates} which do not have
\begin{figure}[h]
\centering
\setlength\fboxsep{0pt}
\setlength\fboxrule{0.5pt}
\fbox{\includegraphics[scale=.25]{shift_class_lengths_exp/out_shift_spike_templates_0.png}}
\fbox{\includegraphics[scale=.25]{shift_class_lengths_exp/out_shift_spike_templates_1.png}}
\caption{Templates inferred with shifts}
\label{fig:shift_spike_templates}
\end{figure}
the smearing evident in \autoref{fig:no_shift_templates} since
we allowed shifts during training.



\subsection{Experiment with p versus b}

In this next experiment we compare a classification experiment
with binary edge feaures computed over spectrograms of individuals
saying \texttt{p} versus \texttt{b}.  The labels and recording
came from the TIMIT databse.

We find that there is good performance to be had in classification
when using roughly six templates for each class. We first
establish that longer or shorter templates do not make much 
of a difference at the scale that we are working at, this is 
important because it shows that the information in the boundary
is not all that important for classification and can be
coherently modeled as background.

We can see the result in 
\begin{table}[h]
  \centering
  \begin{tabular}{| l | c | c |  r |}
    \hline
     Frame Length & Shifts & Other Info & Error Rate \\ \hline\hline
     31 & 1 &  & \input{exp/p_b_exp/b_p_1sh_6c_3st_31l.error_rate} \\
     \hline
     37 & 1 &  & \input{exp/p_b_exp/b_p_1sh_6c_0st_37l.error_rate} \\
     \hline
     35 & 3 &  & \input{exp/p_b_exp/b_p_3sh_6c_0st_35l.error_rate} \\
     \hline
     35 & 1 &  & \input{exp/p_b_exp/b_p_1sh_6c_1st_35l.error_rate} \\
     \hline
     31 & 7 &  & \input{exp/p_b_exp/b_p_7sh_6c_0st_31l.error_rate} \\
     \hline
     31 & 7 & \texttt{prev-init} & \input{exp/p_b_exp/b_p_7sh_6c_0st_31l_previnit.error_rate} \\
     \hline
     31 & 5  & \texttt{prev-init}  & \input{exp/p_b_exp/b_p_5sh_6c_1st_31l_previnit.error_rate} \\
     \hline
     31 & 5 & \texttt{prev-init},NI & \input{exp/p_b_exp/b_p_5sh_6c_1st_31l_previnit_ni.error_rate} \\
     \hline
     31 & 7 & NI & \input{exp/p_b_exp/b_p_7sh_6c_0st_31l_normal_independent.error_rate} \\
     \hline
     31 & 7 & ND & \input{exp/p_b_exp/b_p_7sh_6c_0st_31l_normal_dependent.error_rate} \\
     \hline
  \end{tabular}
  \caption{Error rates for shorter and longer templates}
  \label{tab:myfirsttable}
\end{table}
and so we see a non-linear relationship between shifts and error
rates.  It seems that allowing enough shifts (and using a good 
enough initialization) we can equal the performance of the model
without the shift.  On the other hand, we do worse with
badly initialized shifting models or models which do not
allow enough shifts. A word on the codes: \texttt{prev-init} indicates
that we use the model developed with no shifts as the initialization of the templates.  While `NI` indicates that we enforce a 
normal-like distribution over shifts that is independent of class,
so that, contrary to the description in the previous sections we
have $\pi(r,c)=\tau(r)\eta(c) $ where $\tau$ is the normal-like
distribution over shifts and $\eta$ is a distribution over classes.
The estimation equations for $\tau$ and $\eta$ simplify involve
marginalizing over classes and shifts, respectively. 

We also visualize these templates
and then we see them with shifts
\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_1sh_6c_3st_31l_templates_5.png}

\caption{Spectrogram visualization of the \texttt{p} templates with no shifts}
\label{fig:underlying_1sh_6c_3st_31l_templates}
\end{figure}


\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_templates_5.png}

\caption{Spectrogram visualization of the \texttt{p} templates with seven shifts}
\label{fig:underlying_7sh_6c_0st_31l_templates}
\end{figure}

and then we see what happens when we have a good initialization

\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/p_b_exp/p_underlying_7sh_6c_0st_31l_previnit_templates_5.png}

\caption{Spectrogram visualization of the \texttt{p} templates with seven shifts and a good inititalization}
\label{fig:underlying_7sh_6c_0st_31l_previnit_templates}
\end{figure}


\section{\texttt{let} Experiment}

In this experiment we are working on detecting individuals saying
\texttt{let} in switchboard this is because it is not so frequent
that it is guaranteed to be in every sentence and its short enough
that an inflexible template should be able to handle the length variation (since 90\% of the examples are shorter than quarter of a second) . However, we do not have good
transcriptions since the alignment was done automatically
using a forced aligner.  The first parameter that needs to be
set is the template length, since all of our algorithms
need a length  as an input.  The length parameter is essentially
an upperbound on the length of the model. From the histogram
in \autoref{fig:train_let_lengths_hist}
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/train_let_lengths_hist.png}
\caption{Histogram over the lengths of \texttt{let} in swbd}
\label{fig:train_let_lengths_hist}
\end{figure}
we see that \texttt{.4} seconds will cover most of the instances.
We can include a longer template if need be.  We now examine
the templates learned when we extract the data and train templates
over the specified length.  There are notably two strategies 
to extracting a data window of fixed length: the first is to extract
the window from the hypothesized beginning and the second
is to extract the window from the hypothesized middle.  We opt
for the latter type and will allow shifts so we extracting a .5 second window around the middle of the data.
To get a sense of the different templates that are learned over
\texttt{let}
here we can see a 4-class visualization of templates
trained with one shift in \autoref{fig:let_underlying_viz_1sh_4c_3st_54l}, 
three shifts in \autoref{fig:let_underlying_viz_3sh_4c_0st_58l_prev_init}, 
and then seven shifts in \autoref{fig:let_underlying_viz_7sh_4c_0st_54l_prev_init}:
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_underlying_templates_0.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_underlying_templates_1.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_underlying_templates_2.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_underlying_templates_3.png}
\caption{Visualization of let-clusters with 1 shift}
\label{fig:let_underlying_viz_1sh_4c_3st_54l}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_3sh_4c_0st_58l_prev_init_underlying_templates_0.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_3sh_4c_0st_58l_prev_init_underlying_templates_1.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_3sh_4c_0st_58l_prev_init_underlying_templates_2.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_3sh_4c_0st_58l_prev_init_underlying_templates_3.png}
\caption{Visualization of let-clusters with 3 shifts}
\label{fig:let_underlying_viz_3sh_4c_0st_58l_prev_init}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_underlying_templates_0.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_underlying_templates_1.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_underlying_templates_2.png}
\includegraphics[scale=.5]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_underlying_templates_3.png}
\caption{Visualization of let-clusters with 7 shifts}
\label{fig:let_underlying_viz_7sh_4c_0st_54l_prev_init}
\end{figure}
and then looking at the edge templates with seven shifts (\autoref{fig:let_7sh_4c_0st_54l_prev_init_templates}) compared with 1 shift (\autoref{fig:let_1sh_4c_3st_54l_templates})
\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_5.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_6.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_7sh_4c_0st_54l_prev_init_templates_0_7.png}
\caption{Visualization of let-edge templates with 7 shifts}
\label{fig:let_7sh_4c_0st_54l_prev_init_templates}
\end{figure}
\begin{figure}[h]
\centering
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_0.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_1.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_2.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_3.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_4.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_5.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_6.png}
\includegraphics[scale=.25]{/home/mark/Research/Spoken_Term_Detection/shiftem/exp/let_exp/let_1sh_4c_3st_54l_templates_0_7.png}
\caption{Visualization of let-edge templates with 1 shift}
\label{fig:let_1sh_4c_3st_54l_templates}
\end{figure}



\subsubsection{Single Template with Shift}

Here we investigate what happens if we have a single class
with shift.  We compare it to the case where the template is short
\begin{verbatim}
cat << "EOF" > b_p_1c_3s_35l.config
[TRAINDATA]
num_training=100
template_lengths=(10,20)
template_means=.9
background_means=.2
vector_length=40
num_features=16
num_shifts=9
random_seed=0
num_classes=2
template_type='uniform'
num_curves=4
template_blur_sigma=.3

[EMTRAINING]
tolerance=1e-6
num_shifts=3
num_classes=1
start_time=0
template_length=35
min_prob=.01
class_shift_min_prob=.005
random_seed=0

[INFERENCE]
start_time=0
num_shifts=3


EOF
\end{verbatim}
and then we do five shifts
\begin{verbatim}
cat << "EOF" > b_p_1c_5s_33l.config
[TRAINDATA]
num_training=100
template_lengths=(10,20)
template_means=.9
background_means=.2
vector_length=40
num_features=16
num_shifts=9
random_seed=0
num_classes=2
template_type='uniform'
num_curves=4
template_blur_sigma=.3

[EMTRAINING]
tolerance=1e-6
num_shifts=5
num_classes=1
start_time=0
template_length=33
min_prob=.01
class_shift_min_prob=.005
random_seed=0

[INFERENCE]
start_time=0
num_shifts=5


EOF
\end{verbatim}

and we run the same experiment but with a single shift
and where we have the start time put the shorter template
in the middle
\begin{verbatim}
cat << "EOF" > b_p_1c_1s_35l.config
[TRAINDATA]
num_training=100
template_lengths=(10,20)
template_means=.9
background_means=.2
vector_length=40
num_features=16
num_shifts=9
random_seed=0
num_classes=2
template_type='uniform'
num_curves=4
template_blur_sigma=.3

[EMTRAINING]
tolerance=1e-6
num_shifts=1
num_classes=1
start_time=1
template_length=35
min_prob=.01
class_shift_min_prob=.005
random_seed=0

[INFERENCE]
start_time=1
num_shifts=1


EOF
\end{verbatim}
and then we do the same but with a longer template
\begin{verbatim}
cat << "EOF" > b_p_1c_1s_37l.config
[TRAINDATA]
num_training=100
template_lengths=(10,20)
template_means=.9
background_means=.2
vector_length=40
num_features=16
num_shifts=9
random_seed=0
num_classes=2
template_type='uniform'
num_curves=4
template_blur_sigma=.3

[EMTRAINING]
tolerance=1e-6
num_shifts=1
num_classes=1
start_time=0
template_length=37
min_prob=.01
class_shift_min_prob=.005
random_seed=0

[INFERENCE]
start_time=0
num_shifts=1

EOF
\end{verbatim}

and then we run the experiment to train them

\begin{verbatim}
for suff in 1c_1s_37l 1c_3s_35l 1c_5s_33l 1c_1s_35l ; do
    src/bernoullishiftonly_em.py -c b_p_${suff}.config -i data/b.npy\
        -o b_p_exp/out_b_${suff}_ --visualize_templates
    src/bernoullishiftonly_em.py -c b_p_${suff}.config -i data/p_train.npy\
        -o b_p_exp/out_p_${suff}_ --visualize_templates
done
\end{verbatim}
and then we test these on some development data
\begin{verbatim}
for suff in 1c_1s_37l 1c_3s_35l 1c_5s_33l 1c_1s_35l ; do
    echo $suff
    src/bernoullishift_binary_classify.py -c b_p_${suff}.config \
        --models b_p_exp/out_b_${suff}_templates.npy\
                 b_p_exp/out_p_${suff}_templates.npy\
        --data data/b_dev.npy data/p_dev.npy\
        --out b_p_exp/b_p_${suff}_results_\
        --bgds b_p_exp/out_b_${suff}_background.npy\
               b_p_exp/out_p_${suff}_background.npy
done
\end{verbatim}
\subsubsection{Multiple Templates}

We create a variety of different template lengths
with different numbers of mixture components and different
numbers of shifts
\begin{verbatim}
for num_shifts in 1 3 5 7 ; do
use_length=$(( 37 - $num_shifts + 1 ))
for num_classes in 1 2 3 4 5 6 7 8 9 ; do
echo $num_shifts $use_length $num_classes
echo -e "[TRAINDATA]
num_training=100
template_lengths=(10,20)
template_means=.9
background_means=.2
vector_length=40
num_features=16
num_shifts=9
random_seed=0
num_classes=2
template_type='uniform'
num_curves=4
template_blur_sigma=.3

[EMTRAINING]
tolerance=1e-6
num_shifts=${num_shifts}
num_classes=${num_classes}
start_time=0
template_length=${use_length}
min_prob=.01
class_shift_min_prob=.005
random_seed=0

[INFERENCE]
start_time=0
num_shifts=${num_shifts}" > b_p_${num_classes}c_${num_shifts}s_${use_length}l.config

done
done

\end{verbatim}
We compare p versus b in this experiment.  The data we consider
is contained in 
\begin{verbatim}
../../phoneclassification/data/local/data/b_train_examples.npy
../../phoneclassification/data/local/data/p_train_examples.npy
\end{verbatim}
and we will train templates from these two and then see
if the templates can help with learning and the SVM
\begin{verbatim}
mkdir -p b_p_exp/
for num_shifts in 1 3 5 7 ; do
use_length=$(( 37 - $num_shifts + 1 ))
for num_classes in 2 3 4 5 6 7 8 9 ; do
suff=${num_classes}c_${num_shifts}s_${use_length}l
echo $suff
echo 
src/bernoullishiftonly_em.py -c b_p_${suff}.config -i data/b.npy -o b_p_exp/out_b_${suff}_ --visualize_templates
src/bernoullishiftonly_em.py -c b_p_${suff}.config -i data/p_train.npy -o b_p_exp/out_p_${suff}_ --visualize_templates
done
done
\end{verbatim}
and then we test the performance of these different models
on the dev set and compare it to running the support vector
machine.


To do this we first setup a test set that is drawn from the
phone classification experiment
\begin{verbatim}
../../phoneclassification/data/local/data/b_dev_examples.npy
../../phoneclassification/data/local/data/p_dev_examples.npy
\end{verbatim}
which are saved to
\begin{verbatim}
data/b_dev.npy
data/p_dev.npy
\end{verbatim}
and we
 run the commands
\begin{verbatim}
out_results_fl=b_p_exp/b_p_dev_results.txt
rm -f $out_results_fl
for num_shifts in 1 3 5 7 ; do
use_length=$(( 37 - $num_shifts + 1 ))
for num_classes in 1 2 3 4 5 6 7 8 9 ; do
suff=${num_classes}c_${num_shifts}s_${use_length}l

if [ -e b_p_exp/out_b_${suff}_templates.npy ] ; then
echo $suff
echo $suff `src/bernoullishift_binary_classify.py -c b_p_${suff}.config  --models b_p_exp/out_b_${suff}_templates.npy b_p_exp/out_p_${suff}_templates.npy --data data/b_dev.npy data/p_dev.npy --out b_p_exp/b_p_${suff}_results_ --bgds b_p_exp/out_b_${suff}_background.npy b_p_exp/out_p_${suff}_background.npy | awk '{ print $3 }'`  >> $out_results_fl
fi
done
done
\end{verbatim}
and in doing this we find that shifts do not seem to help very
much since the data is already well-centerd.  Its possible
that it might help in other ways, though.

\begin{verbatim}
out_results_fl=b_p_exp/b_p_dev_results.txt
rm -f $out_results_fl
for num_shifts in 7 ; do
use_length=$(( 37 - $num_shifts + 1 ))
for num_classes in 1 2 3 4 5 6 7 8 9 ; do
suff=${num_classes}c_${num_shifts}s_${use_length}l

if [ -e b_p_exp/out_b_${suff}_templates.npy ] ; then
echo $suff
echo $suff `src/bernoullishift_binary_classify.py -c b_p_${suff}.config  --models b_p_exp/out_b_${suff}_templates.npy b_p_exp/out_p_${suff}_templates.npy --data data/b_dev.npy data/p_dev.npy --out b_p_exp/b_p_${suff}_results_ --bgds b_p_exp/out_b_${suff}_background.npy b_p_exp/out_p_${suff}_background.npy | awk '{ print $3 }'`  >> $out_results_fl
fi
done
done
\end{verbatim}
and then we convert this results file into a graphic showing
\begin{verbatim}
src/results_to_graphics.py -i b_p_exp/b_p_dev_results.txt -o b_p_exp/b_p_dev_results
\end{verbatim}

\subsubsection{ Comparisons with respect to fixed lengths}

In the above approach we are taking large quantities of context
along with the data, one question is what happens if the shiftable
template is the same length as the non-shiftable template.
Another question is at what length does the performance begin
to drop.

Here we are considering classification performance where the
length of the template is fixed.  However, we want the
template to be centered over the utterance, and allowed to
shift around the center.  At present we have data that is
37 time units long, so a template with 1 shift is centered
\begin{verbatim}

for base_shifts in 1 3 5 7 9 ; do
use_length=$(( 37 - $base_shifts + 1 ))
tail_length=$(( (37 - $use_length)/2 ))
for num_shifts in `seq 1 2 $base_shifts` ; do
start_time=$(( tail_length - ( $num_shifts - 1)/2  ))
for num_classes in 1 2 3 4 5 6 7 8 9 ; do
echo $num_shifts $use_length $num_classes $start_time
echo -e "[TRAINDATA]
num_training=100
template_lengths=(10,20)
template_means=.9
background_means=.2
vector_length=40
num_features=16
num_shifts=9
random_seed=0
num_classes=2
template_type='uniform'
num_curves=4
template_blur_sigma=.3

[EMTRAINING]
tolerance=1e-6
num_shifts=${num_shifts}
num_classes=${num_classes}
start_time=${start_time}
template_length=${use_length}
min_prob=.01
class_shift_min_prob=.005
random_seed=0

[INFERENCE]
start_time=${start_time}
num_shifts=${num_shifts}" > b_p_${num_classes}c_${num_shifts}s_${use_length}l.config

done
done
done

\end{verbatim}
and then we train all the models
\begin{verbatim}
for base_shifts in 1 3 5 7 9 ; do
use_length=$(( 37 - $base_shifts + 1 ))
tail_length=$(( (37 - $use_length)/2 ))
for num_shifts in `seq 1 2 $base_shifts` ; do
start_time=$(( tail_length - ( $num_shifts - 1)/2  ))
for num_classes in 1 2 3 4 5 6 7 8 9 ; do
echo $num_shifts $use_length $num_classes $start_time
suff=${num_classes}c_${num_shifts}s_${use_length}l
if [ ! -e b_p_exp/out_p_${suff}_likelihoods.npy ] ; then
src/bernoullishiftonly_em.py -c  b_p_${suff}.config  -i data/b.npy -o b_p_exp/out_b_${suff}_ --visualize_templates
src/bernoullishiftonly_em.py -c b_p_${suff}.config -i data/p_train.npy -o b_p_exp/out_p_${suff}_ --visualize_templates
if [  -e b_p_exp/out_p_${suff}_likelihoods.npy ] ; then
echo "Just saved out_p_${suff}_likelihoods.npy"
fi
fi
done
done
done
\end{verbatim}
and then we need to get the results on the development set to
then construct the error-rate computations
\begin{verbatim}
out_results_fl=b_p_exp/b_p_dev_results.txt
rm -f $out_results_fl
for base_shifts in 1 3 5 7 9 ; do
use_length=$(( 37 - $base_shifts + 1 ))
tail_length=$(( (37 - $use_length)/2 ))
for num_shifts in `seq 1 2 $base_shifts` ; do
start_time=$(( tail_length - ( $num_shifts - 1)/2  ))
for num_classes in 1 2 3 4 5 6 7 8 9 ; do
echo $num_shifts $use_length $num_classes $start_time
suff=${num_classes}c_${num_shifts}s_${use_length}l


if [ -e b_p_exp/out_p_${suff}_templates.npy ] ; then
echo $suff
echo $suff `src/bernoullishift_binary_classify.py -c b_p_${suff}.config  --models b_p_exp/out_b_${suff}_templates.npy b_p_exp/out_p_${suff}_templates.npy --data data/b_dev.npy data/p_dev.npy --out b_p_exp/b_p_${suff}_results_ --bgds b_p_exp/out_b_${suff}_background.npy b_p_exp/out_p_${suff}_background.npy | awk '{ print $3 }'`  >> $out_results_fl
fi

done
done
done
\end{verbatim}



\begin{verbatim}
src/results_to_graphics.py -i b_p_exp/b_p_dev_results.txt -o b_p_exp/b_p_dev_results --by_length
\end{verbatim}

\subsection{Detection Performance}

Another experiment is to see the performance when random shifts
are introduced into the training and testing  data.
We assemble some data extracted for phone classification.
We presume to ahve the following files within a 
\texttt{data/} directory
\begin{verbatim}
b_core_test_longer_examples.npy
b_core_test_longer_examples_S.npy
b_core_test_longer.frame_trans
b_core_test_longer_info.npy
b_dev_longer_examples.npy
b_dev_longer_examples_S.npy
b_dev_longer.frame_trans
b_dev_longer_info.npy
b_train_longer_examples.npy
b_train_longer_examples_S.npy
b_train_longer.frame_trans
b_train_longer_info.npy
p_core_test_longer_examples.npy
p_core_test_longer_examples_S.npy
p_core_test_longer.frame_trans
p_core_test_longer_info.npy
p_dev_longer_examples.npy
p_dev_longer_examples_S.npy
p_dev_longer.frame_trans
p_dev_longer_info.npy
p_train_longer_examples.npy
p_train_longer_examples_S.npy
p_train_longer.frame_trans
p_train_longer_info.npy
\end{verbatim}
we take the central 37 frames and shift the window around by 5 in 
both directions. To do this we run
\begin{verbatim}
local/extract_randomly_shifted_dataset.py
\end{verbatim}
and the main things we need here is an appropriate config
file with a \texttt{TRAINDATA} section as follows
this section will delineate the length of the template.  The logic
is that we will do a variety of experiments where the data
is shifted randomly and uniformly over an interval of some length
(which will be denoted by \texttt{base\_shifts}). The length
of the data extracted, \texttt{data\_length}, is
one less than the \texttt{template\_length} plus the
\texttt{base\_shifts}.  We have a \texttt{centered\_start}
as the location of where the object begins if it is centered
in the utterance segment.  So the model is that the data 
$\mathcal{D}$ consists of data points $D_i\in\mathbb{R}^{T\times F}$
where $F$ is the number of features and $T$ is the number of times.
We assume a contiguous window 
$$W=\{ (t,f) \mid T_{start}\leq t\leq T_{start}+L,\; 1\leq f\leq F \}$$
and for a given shift $\tau$ we model as object
$$ D_i(W+\tau) = \{ D_i(t+\tau,f) \mid (t,f)\in W\}$$
and all other locations in $D_i$ as background.  In this experiment
we are fixing the size of this window as \texttt{template\_length}
and we consider different \texttt{start\_times} $T_{start}$
which are computed to handle the fact that we use varying numbers
of shifts. To get the start time we get a \texttt{centered\_start}
which is where the window is if there is only a single shift
considered so that $\tau=0$
\begin{verbatim}
mkdir -p conf

template_length=37
largest_shift=11
data_length=$(( $template_length + $largest_shift -1 ))
centered_start=$(( ($data_length - $template_length)/2 ))
for num_shifts in `seq 1 2 $largest_shift` ; do
start_time=$(( $centered_start - ( $num_shifts - 1)/2  ))
for num_classes in 4 6 8 ; do
echo $base_shifts num_shifts=$num_shifts template_length=$template_length num_classes=$num_classes start_time=${start_time}
echo -e "[TRAINDATA]
random_seed=0
center_template_length=${template_length}
num_shifts=${num_shifts}
start_time=${start_time}

[EMTRAINING]
tolerance=1e-6
num_shifts=${num_shifts}
num_classes=${num_classes}
start_time=${start_time}
template_length=${template_length}
min_prob=.01
class_shift_min_prob=.005
random_seed=0

[INFERENCE]
start_time=${start_time}
num_shifts=${num_shifts}" > conf/b_p_${data_length}dl_${num_classes}c_${num_shifts}s_${template_length}l.config

done
done

\end{verbatim}
and this will cover the variety of different config files
for this comparative experiment. The next step is to actually
generate the data
\begin{verbatim}
template_length=37
largest_shift=11
data_length=$(( $template_length + $largest_shift -1 ))
centered_start=$(( ($data_length - $template_length)/2 ))
for num_shifts in `seq 1 2 $largest_shift` ; do
for phn in p b ; do
for dataset in train dev core_test ; do

local/extract_randomly_shifted_dataset.py -c conf/b_p_${data_length}dl_6c_${num_shifts}s_${template_length}l.config -f data/${phn}_${dataset}_longer_examples.npy --out_data data/${phn}_${dataset}_${data_length}dl_${num_shifts}s_examples.npy  --out_relative_starts data/${phn}_${dataset}_${data_length}dl_${num_shifts}s_relative_starts.npy --out_start_times data/${phn}_${dataset}_${data_length}dl_${num_shifts}s_start_times.npy

done
done
done
\end{verbatim}
and then we train all of the models using the various different
config files
\begin{verbatim}
exp_dir=b_p_shifted_exp
mkdir -p $exp_dir

dataset=train
template_length=37
largest_shift=11
data_length=$(( $template_length + $largest_shift -1 ))
for data_shifts in 1 3 7 11 ; do
echo data_shifts=$data_shifts
for num_shifts in `seq 1 2 $largest_shift` ; do
echo num_shifts=$num_shifts
for num_classes in 4 6 8 ; do
echo $num_shifts $use_length $num_classes
suff=${data_length}dl_${num_classes}c_${num_shifts}s_${template_length}l
echo $suff
for phn in p b ; do
echo $phn

if [ ! -e $exp_dir/out_${phn}_${data_shifts}ds_${suff}_class_shift_probs.npy ] ; then
echo Training $exp_dir/out_${phn}_${data_shifts}ds_${suff}_templates.npy
src/bernoullishiftonly_em.py -c  conf/b_p_${suff}.config  -i data/${phn}_${dataset}_${data_length}dl_${data_shifts}s_examples.npy --out_templates $exp_dir/out_${phn}_${data_shifts}ds_${suff}_templates.npy --out_backgrounds $exp_dir/out_${phn}_${data_shifts}ds_${suff}_backgrounds.npy --out_posteriors $exp_dir/out_${phn}_${data_shifts}ds_${suff}_posteriors.npy --out_class_shift_probs $exp_dir/out_${phn}_${data_shifts}ds_${suff}_class_shift_probs.npy -o $exp_dir/out_${phn}_${data_shifts}ds_${suff}_ --visualize_templates
fi 
done
done
done
done
\end{verbatim}
and then we can see the results on a shifted dataset
\begin{verbatim}
exp_dir=b_p_shifted_exp

out_results_fl=$exp_dir/b_p_dev_results.txt
rm -f $out_results_fl
dataset=train
template_length=37
largest_shift=11
data_length=$(( $template_length + $largest_shift -1 ))
for model_data_shifts in 1 3 7 11 ; do
echo model_data_shifts=$model_data_shifts
for test_data_shifts in 1 3 7 11 ; do
echo test_data_shifts=$test_data_shifts
for num_shifts in `seq 1 2 $largest_shift` ; do
# number of shifts is the number used in training the templaes
echo num_shifts=$num_shifts
for used_shifts in `seq 1 2 $largest_shift` ; do
# used_shifts are the number used for inference given in the config file
echo used_shifts=$used_shifts
for num_classes in 4 6 8 ; do
echo $num_shifts $use_length $num_classes
suff=${data_length}dl_${num_classes}c_${num_shifts}s_${template_length}l
use_suff=${data_length}dl_${num_classes}c_${used_shifts}s_${template_length}l
echo $suff

if [ -e $exp_dir/out_p_${model_data_shifts}ds_${suff}_templates.npy ] ; then
echo Training $exp_dir/out_p_${data_shifts}ds_${suff}_templates.npy
echo $exp_dir/out_b_${data_shifts}ds_${suff}_templates.npy
echo ${test_data_shifts}tds_${model_data_shifts}mds_${used_shifts}us_$suff `src/bernoullishift_binary_classify.py \
    -c conf/b_p_${use_suff}.config \
    --models $exp_dir/out_b_${model_data_shifts}ds_${suff}_templates.npy \
             $exp_dir/out_p_${model_data_shifts}ds_${suff}_templates.npy \
    --data data/b_dev_${data_length}dl_${test_data_shifts}s_examples.npy \
           data/p_dev_${data_length}dl_${test_data_shifts}s_examples.npy \
    --out $exp_dir/b_p_${test_data_shifts}tds_${model_data_shifts}mds_${used_shifts}us_${suff}_results_ \
    --bgds $exp_dir/out_b_${model_data_shifts}ds_${suff}_backgrounds.npy \
           $exp_dir/out_p_${model_data_shifts}ds_${suff}_backgrounds.npy \
    | awk '{ print $3 }'`  >> $out_results_fl

fi
done
done
done
done
done

\end{verbatim}

and we also make another config file that is for training
unshifted models


\subsection{Parts Experiments}

Another experiment to run is a patchwork of parts experiment.


\subsection{Artificially Shifting the Data Around}

The next question is whether the shifting helps
in the case where the data is artificially shifted.
In this experiment we generate random shifts of the data over
a data window and then see what happens.

\begin{verbatim}
src/shift_data.py --in data/b.npy -out data/b_shifted.npy -c b_p_5c_5s.config
\end{verbatim}

\subsection{Bernoulli Shift Only EM}

When you run \texttt{src/bernoullishiftonly\_em.py} the assumption
is that you have generated data and a configuration file as
mentioned above.  Looking at the function file establish that 
\texttt{shift\_probs} is non-negative and sums to one and is a uniform distribution, and the variable \texttt{posteriors} should 
be initialized to have a uniform distribution.  We also have that 
\texttt{bgd\_prob=.05} and the template \texttt{template} is a 
one-dimensional array of length \texttt{10} where each entry
is \texttt{.95}.

The next section to get the background sums, \texttt{bgd\_sums}. The
function used for this computation is \texttt{prep\_bgd\_sums}.
We want to ensure that it has the appropriate behavior. To do this
we hand compute the output. We check that 
\begin{verbatim}
np.abs(bgd_sums[0,-1] - bgd_sums[0,shift_id+template_length] -\
   (0 if shift_id == 4 else (X[0]*np.log(bgd_prob/(1-bgd_prob)) + np.log(1-bgd_prob))[X.shape[1]-4+shift_id:].sum())) < 1e-6
\end{verbatim}



\end{document}
